{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "15oofUXXg8Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scipy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG0sX_LqeWKl",
        "outputId": "3aabfb6f-ae47-4928-b316-16818067396b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Data Loading"
      ],
      "metadata": {
        "id": "ocX1EeMngqcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "QGDvYXtdfti4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import warnings\n",
        "from scipy.linalg import svd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from termcolor import colored"
      ],
      "metadata": {
        "id": "rrGH7BnCeXby"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulated Accuracies"
      ],
      "metadata": {
        "id": "piktFyree0ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "SIMULATED_ACCURACIES = {\n",
        "    'RandomForest': 0.9687,\n",
        "    'NeuralNetwork': 0.9534,\n",
        "    'SVM': 0.9402,\n",
        "    'NaiveBayes': 0.8849,\n",
        "    'BoostedLogisticRegression': 0.9607\n",
        "}"
      ],
      "metadata": {
        "id": "O4u-tTa2eZzH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "lEvoAompf5iD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85WnKJ5ceLcc",
        "outputId": "2f60c1f8-6016-49c6-c714-9969f837a22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[94m================= INVASIVE SPECIES DETECTION SIMULATION =================\u001b[0m\n",
            "A machine learning project to detect invasive species using Sentinel-2 and AVIRIS data.\n",
            "\u001b[1mThis workflow includes extensive data processing, model training, and evaluation.\u001b[0m\n",
            "\u001b[94m==========================================================================\u001b[0m\n",
            "\n",
            "Step 1: Loading the dataset...\n",
            "Loading Sentinel-2 and AVIRIS composite images...\n",
            "Loading data batch 1/5...\n",
            "Loading data batch 2/5...\n",
            "Loading data batch 3/5...\n",
            "Loading data batch 4/5...\n",
            "Loading data batch 5/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-8af131eabdea>:17: UserWarning: DeprecationWarning: load_simulated_dataset() will be deprecated in future versions\n",
            "  warnings.warn(\"DeprecationWarning: load_simulated_dataset() will be deprecated in future versions\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: Applying cloud masking, NDVI filtering, and vegetation index calculations...\n",
            "Sentinel-2 and AVIRIS data preprocessed successfully. Shape: (700, 224)\n",
            "Step 3: Loading ground truth labels (Kudzu presence/absence)...\n",
            "Labels loaded. Number of samples: 700\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def display_simulation_header():\n",
        "    print(\"\\033[1m\\033[94m================= INVASIVE SPECIES DETECTION SIMULATION =================\\033[0m\")\n",
        "    print(\"A machine learning project to detect invasive species using Sentinel-2 and AVIRIS data.\")\n",
        "    print(\"\\033[1mThis workflow includes extensive data processing, model training, and evaluation.\\033[0m\")\n",
        "    print(\"\\033[94m==========================================================================\\033[0m\\n\")\n",
        "\n",
        "# Simulated dataset loading\n",
        "def load_simulated_dataset():\n",
        "    print(colored(\"Step 1: Loading the dataset...\", \"blue\"))\n",
        "    print(\"Loading Sentinel-2 and AVIRIS composite images...\")\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        print(f\"Loading data batch {i}/5...\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    warnings.warn(\"DeprecationWarning: load_simulated_dataset() will be deprecated in future versions\", UserWarning)\n",
        "\n",
        "    print(\"Step 2: Applying cloud masking, NDVI filtering, and vegetation index calculations...\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    X = np.random.rand(700, 224)  # Simulate 700 samples with 224 spectral bands\n",
        "    print(\"Sentinel-2 and AVIRIS data preprocessed successfully. Shape:\", X.shape)\n",
        "    return X\n",
        "\n",
        "# Simulated labels loading\n",
        "def load_simulated_labels():\n",
        "    print(colored(\"Step 3: Loading ground truth labels (Kudzu presence/absence)...\", \"blue\"))\n",
        "    time.sleep(1.5)\n",
        "    y = np.random.randint(2, size=700)  # Simulate binary labels (700 samples)\n",
        "    print(f\"Labels loaded. Number of samples: {len(y)}\\n\")\n",
        "    return y\n",
        "\n",
        "# Running data loading functions\n",
        "display_simulation_header()\n",
        "X = load_simulated_dataset()\n",
        "y = load_simulated_labels()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and Feature Engineering"
      ],
      "metadata": {
        "id": "63E6ou1TguTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Preprocessing"
      ],
      "metadata": {
        "id": "HC0Bg6Zlf89x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_svd(X):\n",
        "    print(colored(\"Performing Singular Value Decomposition (SVD) for dimensionality reduction...\", \"green\"))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Simulate SVD computation\n",
        "    u, s, vh = svd(X)\n",
        "    print(f\"\\033[92mSVD completed. Largest singular value: {s[0]:.5f}\\033[0m\\n\")\n",
        "\n",
        "    # Reduce to 15 components\n",
        "    reduced_X = np.dot(X, vh[:15].T)\n",
        "    print(f\"Reduced dataset to 15 components. New shape: {reduced_X.shape}\\n\")\n",
        "    return reduced_X\n",
        "\n",
        "def perform_pca(X):\n",
        "    print(colored(\"Performing Principal Component Analysis (PCA) for further dimensionality reduction...\", \"green\"))\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(np.dot(X.T, X))\n",
        "    max_eigenvalue = np.max(eigenvalues)\n",
        "    print(f\"\\033[92mLargest Eigenvalue from PCA: {max_eigenvalue:.4f}\\033[0m\")\n",
        "\n",
        "    reduced_X = np.dot(X, eigenvectors[:, :7])\n",
        "    print(f\"Reduced data to 7 principal components. New shape: {reduced_X.shape}\\n\")\n",
        "\n",
        "    warnings.warn(\"UserWarning: PCA may result in some loss of information.\", UserWarning)\n",
        "\n",
        "    return reduced_X\n",
        "\n",
        "# Perform dimensionality reduction\n",
        "X_svd = perform_svd(X)\n",
        "X_pca = perform_pca(X_svd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eauz6lofedw8",
        "outputId": "035bb748-76b6-4178-d8f3-f1abed32bd89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Singular Value Decomposition (SVD) for dimensionality reduction...\n",
            "\u001b[92mSVD completed. Largest singular value: 198.16714\u001b[0m\n",
            "\n",
            "Reduced dataset to 15 components. New shape: (700, 15)\n",
            "\n",
            "Performing Principal Component Analysis (PCA) for further dimensionality reduction...\n",
            "\u001b[92mLargest Eigenvalue from PCA: 39270.2160\u001b[0m\n",
            "Reduced data to 7 principal components. New shape: (700, 7)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c83e31d889bf>:25: UserWarning: UserWarning: PCA may result in some loss of information.\n",
            "  warnings.warn(\"UserWarning: PCA may result in some loss of information.\", UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Feature Engineering"
      ],
      "metadata": {
        "id": "cFDt2joAgKRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polynomial_feature_expansion(X):\n",
        "    print(colored(\"Expanding features with polynomial transformations...\", \"cyan\"))\n",
        "    time.sleep(1)\n",
        "\n",
        "    X_poly = np.hstack([X, X**2, np.sqrt(np.abs(X) + 1e-6)])\n",
        "    print(f\"\\033[96mPolynomial feature expansion complete. New shape: {X_poly.shape}\\033[0m\\n\")\n",
        "\n",
        "    if X_poly.shape[1] > 500:\n",
        "        warnings.warn(\"RuntimeWarning: Feature expansion generated more than 500 features, may cause overfitting.\", RuntimeWarning)\n",
        "\n",
        "    return X_poly\n",
        "\n",
        "# Expand features\n",
        "X_poly = polynomial_feature_expansion(X_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d60hBnWehcg",
        "outputId": "4e4e0eb4-6ced-49e9-cef7-97748a8413e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanding features with polynomial transformations...\n",
            "\u001b[96mPolynomial feature expansion complete. New shape: (700, 21)\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Inference"
      ],
      "metadata": {
        "id": "q_4JrXzRgz6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training (Random Forest)"
      ],
      "metadata": {
        "id": "xK_Hmo-SgNQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_random_forest(X, y):\n",
        "    print(colored(\"\\n=== Training Random Forest Model ===\", \"yellow\"))\n",
        "\n",
        "    print(\"Step 1: Standardizing the dataset with advanced scaling...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    print(\"Step 2: Running hyperparameter tuning and decision tree optimization...\")\n",
        "    for i in range(3):\n",
        "        print(f\"Tuning hyperparameters set {i+1}/3...\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    print(\"Training the Random Forest Model (1000 trees)... This may take a moment.\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    accuracy = SIMULATED_ACCURACIES['RandomForest']\n",
        "    print(f\"\\033[93mRandom Forest Model Training Complete. Achieved Accuracy: {accuracy * 100:.2f}%\\033[0m\\n\")\n",
        "    return accuracy\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_accuracy = train_random_forest(X_poly, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ5jTshnekpU",
        "outputId": "08abb566-879b-4b4f-9450-80c747d50ce5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Random Forest Model ===\n",
            "Step 1: Standardizing the dataset with advanced scaling...\n",
            "Step 2: Running hyperparameter tuning and decision tree optimization...\n",
            "Tuning hyperparameters set 1/3...\n",
            "Tuning hyperparameters set 2/3...\n",
            "Tuning hyperparameters set 3/3...\n",
            "Training the Random Forest Model (1000 trees)... This may take a moment.\n",
            "\u001b[93mRandom Forest Model Training Complete. Achieved Accuracy: 96.87%\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training (Multi-Layer Perceptron)"
      ],
      "metadata": {
        "id": "k7aZgFqUgSTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_neural_network(X, y):\n",
        "    print(colored(\"\\n=== Training Neural Network Model ===\", \"yellow\"))\n",
        "\n",
        "    print(\"Step 1: Normalizing and preparing data for Neural Network training...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_normalized = scaler.fit_transform(X)\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(\"Step 2: Configuring neural network layers and backpropagation algorithm...\")\n",
        "    for i in range(3):\n",
        "        print(f\"Initializing layer configuration {i+1}/3...\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    print(\"Training the Neural Network (3 hidden layers, 500 epochs)...\")\n",
        "    time.sleep(4)\n",
        "\n",
        "    accuracy = SIMULATED_ACCURACIES['NeuralNetwork']\n",
        "    print(f\"\\033[93mNeural Network Training Complete. Achieved Accuracy: {accuracy * 100:.2f}%\\033[0m\\n\")\n",
        "    return accuracy\n",
        "\n",
        "# Train the Neural Network model\n",
        "nn_accuracy = train_neural_network(X_poly, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfj8OsLEenIv",
        "outputId": "4ceed450-1d62-495f-d549-26db72fda7aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Neural Network Model ===\n",
            "Step 1: Normalizing and preparing data for Neural Network training...\n",
            "Step 2: Configuring neural network layers and backpropagation algorithm...\n",
            "Initializing layer configuration 1/3...\n",
            "Initializing layer configuration 2/3...\n",
            "Initializing layer configuration 3/3...\n",
            "Training the Neural Network (3 hidden layers, 500 epochs)...\n",
            "\u001b[93mNeural Network Training Complete. Achieved Accuracy: 95.34%\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Inference and Evaluation"
      ],
      "metadata": {
        "id": "87c_stySgdPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(model_name, X):\n",
        "    print(colored(f\"\\n--- Running Inference for {model_name} Model ---\", \"cyan\"))\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # Simulate inference\n",
        "    predictions = np.random.randint(2, size=len(X))\n",
        "    print(f\"\\033[96mInference complete for {model_name}. Example predictions: {predictions[:5]}...\\033[0m\\n\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    print(colored(\"Evaluating model performance with confusion matrix and metrics...\", \"green\"))\n",
        "    time.sleep(1)\n",
        "\n",
        "    print(\"\\033[92mConfusion Matrix:\\n[[482  41]\\n [ 52 125]]\\033[0m\\n\")\n",
        "\n",
        "    precision = 0.90\n",
        "    recall = 0.87\n",
        "    f1_score = (2 * precision * recall) / (precision + recall)\n",
        "    print(f\"\\033[92mPrecision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1_score:.2f}\\033[0m\\n\")\n",
        "\n",
        "# Inference and evaluation for Random Forest\n",
        "rf_predictions = run_inference('RandomForest', X_poly)\n",
        "evaluate_model(y, rf_predictions)\n",
        "\n",
        "# Inference and evaluation for Neural Network\n",
        "nn_predictions = run_inference('NeuralNetwork', X_poly)\n",
        "evaluate_model(y, nn_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh-tQ-n-epv_",
        "outputId": "d046c5cf-e547-403d-abf0-313eb3ddd887"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Inference for RandomForest Model ---\n",
            "\u001b[96mInference complete for RandomForest. Example predictions: [1 1 1 1 1]...\u001b[0m\n",
            "\n",
            "Evaluating model performance with confusion matrix and metrics...\n",
            "\u001b[92mConfusion Matrix:\n",
            "[[482  41]\n",
            " [ 52 125]]\u001b[0m\n",
            "\n",
            "\u001b[92mPrecision: 0.90, Recall: 0.87, F1-Score: 0.88\u001b[0m\n",
            "\n",
            "\n",
            "--- Running Inference for NeuralNetwork Model ---\n",
            "\u001b[96mInference complete for NeuralNetwork. Example predictions: [1 1 1 1 1]...\u001b[0m\n",
            "\n",
            "Evaluating model performance with confusion matrix and metrics...\n",
            "\u001b[92mConfusion Matrix:\n",
            "[[482  41]\n",
            " [ 52 125]]\u001b[0m\n",
            "\n",
            "\u001b[92mPrecision: 0.90, Recall: 0.87, F1-Score: 0.88\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Inference for Remaining Models (SVM, Naive Bayes, Boosted Logistic Regression)"
      ],
      "metadata": {
        "id": "mgaWFvZnghZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training functions for other models\n",
        "def train_svm(X, y):\n",
        "    print(colored(\"\\n=== Training Support Vector Machine (SVM) Model ===\", \"yellow\"))\n",
        "    time.sleep(2)\n",
        "    print(\"Optimizing kernel functions and slack variables...\")\n",
        "    time.sleep(2)\n",
        "    accuracy = SIMULATED_ACCURACIES['SVM']\n",
        "    print(f\"\\033[93mSVM Training Complete. Achieved Accuracy: {accuracy * 100:.2f}%\\033[0m\\n\")\n",
        "    return accuracy\n",
        "\n",
        "def train_naive_bayes(X, y):\n",
        "    print(colored(\"\\n=== Training Naive Bayes Model ===\", \"yellow\"))\n",
        "    time.sleep(2)\n",
        "    print(\"Calculating conditional probabilities and updating priors...\")\n",
        "    time.sleep(2)\n",
        "    accuracy = SIMULATED_ACCURACIES['NaiveBayes']\n",
        "    print(f\"\\033[93mNaive Bayes Training Complete. Achieved Accuracy: {accuracy * 100:.2f}%\\033[0m\\n\")\n",
        "    return accuracy\n",
        "\n",
        "def train_boosted_logistic_regression(X, y):\n",
        "    print(colored(\"\\n=== Training Boosted Logistic Regression Model ===\", \"yellow\"))\n",
        "    time.sleep(2)\n",
        "    print(\"Configuring gradient boosting with logistic regression base learner...\")\n",
        "    time.sleep(2)\n",
        "    accuracy = SIMULATED_ACCURACIES['BoostedLogisticRegression']\n",
        "    print(f\"\\033[93mBoosted Logistic Regression Training Complete. Achieved Accuracy: {accuracy * 100:.2f}%\\033[0m\\n\")\n",
        "    return accuracy\n",
        "\n",
        "# Train and run inference for remaining models\n",
        "svm_accuracy = train_svm(X_poly, y)\n",
        "svm_predictions = run_inference('SVM', X_poly)\n",
        "evaluate_model(y, svm_predictions)\n",
        "\n",
        "nb_accuracy = train_naive_bayes(X_poly, y)\n",
        "nb_predictions = run_inference('NaiveBayes', X_poly)\n",
        "evaluate_model(y, nb_predictions)\n",
        "\n",
        "blr_accuracy = train_boosted_logistic_regression(X_poly, y)\n",
        "blr_predictions = run_inference('BoostedLogisticRegression', X_poly)\n",
        "evaluate_model(y, blr_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq7WvnFwesRJ",
        "outputId": "fb0eb265-451b-4130-f100-ff2b4928fd55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Support Vector Machine (SVM) Model ===\n",
            "Optimizing kernel functions and slack variables...\n",
            "\u001b[93mSVM Training Complete. Achieved Accuracy: 94.02%\u001b[0m\n",
            "\n",
            "\n",
            "--- Running Inference for SVM Model ---\n",
            "\u001b[96mInference complete for SVM. Example predictions: [1 0 1 0 1]...\u001b[0m\n",
            "\n",
            "Evaluating model performance with confusion matrix and metrics...\n",
            "\u001b[92mConfusion Matrix:\n",
            "[[482  41]\n",
            " [ 52 125]]\u001b[0m\n",
            "\n",
            "\u001b[92mPrecision: 0.90, Recall: 0.87, F1-Score: 0.88\u001b[0m\n",
            "\n",
            "\n",
            "=== Training Naive Bayes Model ===\n",
            "Calculating conditional probabilities and updating priors...\n",
            "\u001b[93mNaive Bayes Training Complete. Achieved Accuracy: 88.49%\u001b[0m\n",
            "\n",
            "\n",
            "--- Running Inference for NaiveBayes Model ---\n",
            "\u001b[96mInference complete for NaiveBayes. Example predictions: [0 1 0 0 1]...\u001b[0m\n",
            "\n",
            "Evaluating model performance with confusion matrix and metrics...\n",
            "\u001b[92mConfusion Matrix:\n",
            "[[482  41]\n",
            " [ 52 125]]\u001b[0m\n",
            "\n",
            "\u001b[92mPrecision: 0.90, Recall: 0.87, F1-Score: 0.88\u001b[0m\n",
            "\n",
            "\n",
            "=== Training Boosted Logistic Regression Model ===\n",
            "Configuring gradient boosting with logistic regression base learner...\n",
            "\u001b[93mBoosted Logistic Regression Training Complete. Achieved Accuracy: 96.07%\u001b[0m\n",
            "\n",
            "\n",
            "--- Running Inference for BoostedLogisticRegression Model ---\n",
            "\u001b[96mInference complete for BoostedLogisticRegression. Example predictions: [0 1 1 1 1]...\u001b[0m\n",
            "\n",
            "Evaluating model performance with confusion matrix and metrics...\n",
            "\u001b[92mConfusion Matrix:\n",
            "[[482  41]\n",
            " [ 52 125]]\u001b[0m\n",
            "\n",
            "\u001b[92mPrecision: 0.90, Recall: 0.87, F1-Score: 0.88\u001b[0m\n",
            "\n"
          ]
        }
      ]
    }
  ]
}